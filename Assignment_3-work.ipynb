{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial/data)\n",
    "======\n",
    "\n",
    "## Data Set\n",
    "\n",
    "The labeled data set consists of 50,000 IMDB movie reviews, specially selected for sentiment analysis. The sentiment of reviews is binary, meaning the IMDB rating < 5 results in a sentiment score of 0, and rating >=7 have a sentiment score of 1. No individual movie has more than 30 reviews. The 25,000 review labeled training set does not include any of the same movies as the 25,000 review test set. In addition, there are another 50,000 IMDB reviews provided without any rating labels.\n",
    "\n",
    "## File descriptions\n",
    "\n",
    "labeledTrainData - The labeled training set. The file is tab-delimited and has a header row followed by 25,000 rows containing an id, sentiment, and text for each review.\n",
    "## Data fields\n",
    "\n",
    "* id - Unique ID of each review\n",
    "* sentiment - Sentiment of the review; 1 for positive reviews and 0 for negative reviews\n",
    "* review - Text of the review\n",
    "\n",
    "## Objective\n",
    "Objective of this dataset is base on **review** we predict **sentiment** (positive or negative) so X is **review** column and y is **sentiment** column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset\n",
    "we only forcus on \"labeledTrainData.csv\" file\n",
    "\n",
    "Let's first of all have a look at the data.\n",
    "\n",
    "[Click here to download dataset](https://s3-ap-southeast-1.amazonaws.com/ml101-khanhnguyen/week3/assignment/labeledTrainData.tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas, numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12262</th>\n",
       "      <td>249_10</td>\n",
       "      <td>1</td>\n",
       "      <td>I have seen the trailer for this movie several...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9890</th>\n",
       "      <td>9100_7</td>\n",
       "      <td>1</td>\n",
       "      <td>Spanish films are into a, if not Golden, defin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6790</th>\n",
       "      <td>6188_1</td>\n",
       "      <td>0</td>\n",
       "      <td>This is by far the worst adaptation of Jane Ey...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22092</th>\n",
       "      <td>936_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I don't understand people. Why is it that this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17750</th>\n",
       "      <td>7840_8</td>\n",
       "      <td>1</td>\n",
       "      <td>BORN TO BOOGIE is a real 'find'--though a rock...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6019</th>\n",
       "      <td>888_4</td>\n",
       "      <td>0</td>\n",
       "      <td>This was a movie i could not wait to see! So i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8716</th>\n",
       "      <td>1431_3</td>\n",
       "      <td>0</td>\n",
       "      <td>!!!!! POSSIBLE SPOILER !!!!!&lt;br /&gt;&lt;br /&gt;You`d ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>12009_3</td>\n",
       "      <td>0</td>\n",
       "      <td>I had a video of the thing. And I think it was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8109</th>\n",
       "      <td>7620_1</td>\n",
       "      <td>0</td>\n",
       "      <td>I rented this movie because it sounded pretty ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7632</th>\n",
       "      <td>11597_4</td>\n",
       "      <td>0</td>\n",
       "      <td>I will stat of with the plot Alice, having sur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  sentiment                                             review\n",
       "12262   249_10          1  I have seen the trailer for this movie several...\n",
       "9890    9100_7          1  Spanish films are into a, if not Golden, defin...\n",
       "6790    6188_1          0  This is by far the worst adaptation of Jane Ey...\n",
       "22092    936_1          0  I don't understand people. Why is it that this...\n",
       "17750   7840_8          1  BORN TO BOOGIE is a real 'find'--though a rock...\n",
       "6019     888_4          0  This was a movie i could not wait to see! So i...\n",
       "8716    1431_3          0  !!!!! POSSIBLE SPOILER !!!!!<br /><br />You`d ...\n",
       "820    12009_3          0  I had a video of the thing. And I think it was...\n",
       "8109    7620_1          0  I rented this movie because it sounded pretty ...\n",
       "7632   11597_4          0  I will stat of with the plot Alice, having sur..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read dataset with extra params sep='\\t', encoding=\"latin-1\"\n",
    "labeledTrainData = pd.read_csv(\"labeledTrainData.csv\",sep='\\t',encoding=\"latin-1\")\n",
    "labeledTrainData.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/dks/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 287025),\n",
       " ('a', 155092),\n",
       " ('and', 152651),\n",
       " ('of', 142970),\n",
       " ('to', 132568),\n",
       " ('is', 103227),\n",
       " ('in', 85576),\n",
       " ('I', 65971),\n",
       " ('that', 64555),\n",
       " ('this', 57195),\n",
       " ('it', 54416),\n",
       " ('/><br', 50935),\n",
       " ('was', 46697),\n",
       " ('as', 42509),\n",
       " ('with', 41718),\n",
       " ('for', 41067),\n",
       " ('but', 33783),\n",
       " ('The', 33760),\n",
       " ('on', 30765),\n",
       " ('movie', 30496)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "vocab = Counter()\n",
    "\n",
    "for twit in labeledTrainData.review:\n",
    "    for word in twit.split(' '):\n",
    "        vocab[word] += 1\n",
    "\n",
    "vocab.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 65971),\n",
       " ('/><br', 50935),\n",
       " ('The', 33760),\n",
       " ('movie', 30496),\n",
       " ('film', 27394),\n",
       " ('one', 20685),\n",
       " ('like', 18133),\n",
       " ('This', 12279),\n",
       " ('would', 11922),\n",
       " ('good', 11435),\n",
       " ('It', 10950),\n",
       " ('really', 10814),\n",
       " ('even', 10605),\n",
       " ('see', 10154),\n",
       " ('-', 9355),\n",
       " ('get', 8776),\n",
       " ('story', 8523),\n",
       " ('much', 8506),\n",
       " ('time', 7762),\n",
       " ('make', 7485)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "vocab_reduced = Counter()\n",
    "\n",
    "for w, c in vocab.items():\n",
    "    if not w in stop:\n",
    "        vocab_reduced[w]=c\n",
    "\n",
    "vocab_reduced.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters and \"trash\"\n",
    "import re\n",
    "def preprocessor(text):\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text) # Your code here\n",
    "    \n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Your code here\n",
    "    \n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    # Your code here\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer and stemming\n",
    "# tokenizer: to break down our twits in individual words\n",
    "# stemming: reducing a word to its root\n",
    "from nltk.stem import PorterStemmer\n",
    "# Your code here\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()] # Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test\n",
    "# Your code here\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = labeledTrainData['review']\n",
    "y = labeledTrainData['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model and Train \n",
    "\n",
    "Using **Pipeline** to concat **tfidf** step and **LogisticRegression** step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', 'onc', 'onli', 'ourselv', 'themselv', 'thi', 'veri', 'wa', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\Anaconda\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function prepr...e, penalty='l2', random_state=0, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Pipeline, LogisticRegression, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words=stop,\n",
    "                        tokenizer=tokenizer_porter,\n",
    "                        preprocessor=preprocessor)# Your code here\n",
    "\n",
    "clf = Pipeline([('vect', tfidf),\n",
    "                ('clf', LogisticRegression(random_state=0))])# Your code here)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8850666666666667\n",
      "confusion matrix:\n",
      " [[3293  503]\n",
      " [ 359 3345]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.87      0.88      3796\n",
      "           1       0.87      0.90      0.89      3704\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      7500\n",
      "   macro avg       0.89      0.89      0.89      7500\n",
      "weighted avg       0.89      0.89      0.89      7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Test dataset to evaluate model\n",
    "# classification_report\n",
    "# confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is really not bad --> Negative, Positive = [0.99610592 0.00389408]\n",
      "I dont love this! --> Negative, Positive = [0.26942903 0.73057097]\n",
      ":( :) --> Negative, Positive = [0.33408938 0.66591062]\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"This is really not bad\",\n",
    "    \"I dont love this!\",\n",
    "    \":( :)\",\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pickle to export our trained model\n",
    "import pickle\n",
    "import os\n",
    "pickle.dump(clf, open(os.path.join('data', 'Assignment3worklogisticRegression.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
